# Проект: Классификация видов спорта с использованием сверточных нейронных сетей 

Для обучения и тестирования использован датасет с Kaggle:  
[Sports Image Classification Dataset](https://www.kaggle.com/datasets/gpiosenka/sports-classification).

Мы поставили цель:  
**Сравнить две архитектуры нейронных сетей** и определить, какая лучше подходит для этой задачи:
- **SimpleCNN** — простая сверточная нейронная сеть.
- **CNNWithResidual** — сверточная сеть с остаточными (residual) блоками.

Состав проекта:
- `dataset.py` — подготовка датасета, аугментации.
- `models.py` — определение моделей (SimpleCNN, CNNWithResidual).
- `trainer.py` — функции обучения моделей.
- `utils.py` — вспомогательные утилиты (построение графиков, подсчет параметров).
- `experiments_simple_cnn.ipynb` — эксперименты с SimpleCNN.
- `experiments_cnn_with_res.ipynb` — эксперименты с CNNWithResidual.
- `final_model_run.ipynb` — запуск финальной выбранной модели.

Начнем с простой сверточной сети. (Все эксперименты с ней просиходили в файле - experiments_simple_cnn)

В архитектуре `SimpleCNN` для активации используется функция **ReLU (Rectified Linear Unit)** после каждого сверточного слоя.  
Для стабилизации обучения после сверточных слоев применяется **Batch Normalization**, что помогает ускорить сходимость.  
Для борьбы с переобучением в сети используется **Dropout**. 

В первую очередь мы подобрали оптимальную глубину для простой сверточной сети. Были рассмотрены три варианта: глубина 2, 4 и 6 сверточных слоев. Все модели обучались на одном и том же датасете в течение 5 эпох для корректного сравнения.\
Глубина 2.\
<img width="1200" height="400" alt="image" src="https://github.com/user-attachments/assets/550fec79-4181-47bb-adc9-8f95058f311c" />\
Глубина 4.\
<img width="1202" height="403" alt="image" src="https://github.com/user-attachments/assets/8e7eefa7-9259-4949-9aea-097cbd0f4517" />\
Глубина 6.\
<img width="1198" height="399" alt="image" src="https://github.com/user-attachments/assets/d33143e7-54c3-47aa-bef4-c4aca433213e" />\
По графикам видно, что при увеличении глубины до 6 слоев модель показывала стабильный рост точности на тестовой выборке и меньший тестовый лосс, в то время как при меньшей глубине результаты были хуже.\
Вывод: глубина 6 была выбрана как оптимальная, так как обеспечивала наилучший баланс между качеством предсказаний и устойчивостью обучения.

На глубине 6 слоев мы проверили эффективность работы модели при различных размерах ядра свёртки: 3, 5 и 7.\
Время обучения для каждого случая составило:ядро 3: 109.13 сек, ядро 5: 140.82 сек, ядро 7: 604.57 сек.\
Ядро 3x3\
<img width="1194" height="399" alt="image" src="https://github.com/user-attachments/assets/678466b8-911b-49f8-b577-42111c4e2113" />\
Ядро 5x5\
<img width="1198" height="398" alt="image" src="https://github.com/user-attachments/assets/0bfe7350-fb51-48b4-a1ae-bb95889c175a" />\
Ядро 7x7\
<img width="1202" height="399" alt="image" src="https://github.com/user-attachments/assets/9866f1cd-4f3f-4b02-8c94-96ffa31691cf" />\
Лучшие метрики по точности  показали варианты с ядрами 5 и 7, однако из-за существенного увеличения времени обучения при ядре 7 оптимальным решением оказалось использование ядра 5.

Далее модель была запущена на 50 эпохах с глубиной 6 и ядром 5 для оценки её устойчивости. Время обучения - 1705секунд.\
<img width="1192" height="394" alt="image" src="https://github.com/user-attachments/assets/abdff410-a072-4c04-bb50-d10f11a9aabe" />\
По результатам видно, что модель демонстрирует признаки переобучения: training loss продолжает снижаться почти до нуля, а training accuracy достигает 100%, тогда как test loss остаётся колеблющимся,\
а testaccuracy стабилизируется на уровне около 75%. Это говорит о том, что модель слишком хорошо запомнила обучающую выборку и недостаточно обобщает на валидационных данных.

Добавили аугментацию для улучшения её способности к обобщению.\
Применённые аугментации включают:\
RandomResizedCrop — случайная обрезка с изменением масштаба\
RandomHorizontalFlip — случайное горизонтальное отражение\
ColorJitter — изменение яркости, контраста и насыщенности\
RandomRotation — случайный поворот до 15 градусов\
<img width="1184" height="387" alt="image" src="https://github.com/user-attachments/assets/59993963-c320-46ed-9ca6-07c2a3dc7b0e" />\
Результат оказался вполне удовлетворительным: модель демонстрирует стабильное снижение test loss примерно до 1.0 и рост test accuracy до около 70%. Время обучения составило 3155 секунд.














